{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1207d8-6959-4dca-81af-584ba8677218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling aeda25e63ebd: 100% ▕██████████████████▏ 3.3 GB                         \u001b[K\n",
      "pulling e0a42594d802: 100% ▕██████████████████▏  358 B                         \u001b[K\n",
      "pulling dd084c7d92a3: 100% ▕██████████████████▏ 8.4 KB                         \u001b[K\n",
      "pulling 3116c5225075: 100% ▕██████████████████▏   77 B                         \u001b[K\n",
      "pulling b6ae5839783f: 100% ▕██████████████████▏  489 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "! ollama pull gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c28ef293-907e-42e8-97db-8076660a5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 970aa74c0a90: 100% ▕██████████████████▏ 274 MB                         \u001b[K\n",
      "pulling c71d239df917: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling ce4a164fc046: 100% ▕██████████████████▏   17 B                         \u001b[K\n",
      "pulling 31df23ea7daa: 100% ▕██████████████████▏  420 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "! ollama pull nomic-embed-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7c6bbd0-a12e-400e-afac-928104ddc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6eba1aa-7cf0-41d3-b909-e9d28715fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e509a897-6fc6-43a9-a820-4b8b771086a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d54f3e-58ed-42e6-b82b-64152b43285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0266ee56-e4eb-46cf-98a4-5fc563f4184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"gemma3\", base_url=\"http://127.0.0.1:11434\",temperature = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a338c1cd-656f-4edb-831f-9cb7876dcf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://127.0.0.1:11434\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8797ca3b-5d31-475c-a947-b9c9909643e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a711b0ba-161b-4cb9-8038-ad3ab3b4f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_process(files):\n",
    "    system_prompt = \"You are a helpful assistant that provides concise and accurate responses based on provided documents.\"\n",
    "    system_prompt += \"If the customer greets with Hi or hello greet them and if the customer talks anything in general respond based on his message.\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ])\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        file_path = getattr(file, 'name', None) or getattr(file, 'tempfile', None)\n",
    "    \n",
    "        if not file_path:\n",
    "            raise ValueError(\"Invalid file upload\")\n",
    "            \n",
    "        # Load and process PDF\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "            \n",
    "        # Add filename to metadata\n",
    "        for doc in docs:\n",
    "            doc.metadata['filename'] = os.path.basename(file.name).split(\".\")[0]\n",
    "            documents.append(doc)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    for i in chunks:\n",
    "        i.page_content = 'this chunk is from' +\" \"+ i.metadata['filename']+\":\" +\" \"+ i.page_content \n",
    "    db_name = 'vector_db'\n",
    "    \n",
    "    if os.path.exists(db_name):\n",
    "        Chroma(persist_directory=db_name, embedding_function=embed_model).delete_collection()\n",
    "        \n",
    "    vector_store = Chroma.from_documents(documents = chunks, embedding = embed_model, persist_directory=db_name)\n",
    "    collection = vector_store._collection\n",
    "    retriever = vector_store.as_retriever(search_type=\"mmr\",  # Maximal Marginal Relevance\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20})\n",
    "    llm_with_prompt = prompt | llm  # Automatically formats messages then runs LLM\n",
    "    memory = ConversationBufferMemory(memory_key= 'chat_history', return_messages = True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_with_prompt,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=False\n",
    "        )\n",
    "    return conversation_chain\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24708e21-d861-4708-a692-7079c2c31fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_func(messsage, history):\n",
    "    conversation_chain = file_process(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "491c9633-ffb7-4ccb-a9d2-cfc08b3fb338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_func(files):\n",
    "    global current_conversation_chain\n",
    "    conversation_chain = file_process(files)\n",
    "    if conversation_chain:\n",
    "        current_conversation_chain = conversation_chain\n",
    "    return 'File uploaded successfully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dafbced-9155-4063-a146-e2c561cfa63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chat function\n",
    "def chat_func(message, history):\n",
    "    global current_conversation_chain\n",
    "    if current_conversation_chain is None:\n",
    "        return \"Please upload documents first before asking questions.\"\n",
    "    \n",
    "    result = current_conversation_chain.invoke({'question': message})\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97e24f2f-4635-415b-a0ba-4d484326cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/2kj6d1ld1w5f46gd1x6cqh2m0000gn/T/ipykernel_32038/1259904431.py:30: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=500),\n",
      "/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(css=\"\"\"\n",
    "    .gradio-container, .gradio-container > div {\n",
    "        height: 100vh !important;\n",
    "    }\n",
    "    .chat-column {\n",
    "        height: calc(100vh - 140px);\n",
    "    }\n",
    "    .file-column {\n",
    "        height: 50%;\n",
    "    }\"\"\") as demo:\n",
    "    gr.Markdown(\"# RAG Chatbot 🤖\")\n",
    "    gr.Markdown(\"Upload your PDF documents and ask questions about them.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale = 1):\n",
    "            upload_button = gr.File(\n",
    "                label = \"Click to Upload PDFs\", \n",
    "                file_types=[\".pdf\"], \n",
    "                file_count=\"multiple\"\n",
    "            )\n",
    "            file_output = gr.Textbox(label=\"Upload Status\")\n",
    "            upload_button.upload(\n",
    "                upload_func,\n",
    "                inputs=upload_button,\n",
    "                outputs=file_output\n",
    "            )\n",
    "        with gr.Column(scale = 2):\n",
    "            chatbot = gr.ChatInterface(\n",
    "                chat_func,\n",
    "                chatbot=gr.Chatbot(height=500),\n",
    "                #textbox=gr.Textbox(placeholder=\"Ask questions about your documents...\", container=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41b5451b-f75f-4992-a716-d7fc85467690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/2kj6d1ld1w5f46gd1x6cqh2m0000gn/T/ipykernel_32038/1198931100.py:33: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  Chroma(persist_directory=db_name, embedding_function=embed_model).delete_collection()\n",
      "/var/folders/z1/2kj6d1ld1w5f46gd1x6cqh2m0000gn/T/ipykernel_32038/1198931100.py:40: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key= 'chat_history', return_messages = True)\n",
      "Number of requested results 20 is greater than number of elements in index 4, updating n_results = 4\n",
      "Number of requested results 20 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325090b6-65b8-4dd9-a822-e6d697a2489b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409c639-5db7-476f-967d-0d628b1fffe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f0a3e-33db-41f6-b616-73f88f5fda97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
